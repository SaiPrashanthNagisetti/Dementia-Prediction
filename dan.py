# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CoGyZWd6jJvTRbOQ2nypSFHSt3EwJYF
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix
!pip install tensorflow
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load datasets
df_train = pd.read_csv('/content/dataset_train_updated.csv')
df_test = pd.read_csv('/content/dataset_test_updated.csv')

# Separate features and labels
X_train_texts = df_train['text'].values
y_train = df_train['class'].values
X_test_texts = df_test['text'].values
y_test = df_test['class'].values

"""tokenization"""

# Tokenization and padding
tokenizer = Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(X_train_texts)

X_train_sequences = tokenizer.texts_to_sequences(X_train_texts)
X_test_sequences = tokenizer.texts_to_sequences(X_test_texts)

max_length = max(len(seq) for seq in X_train_sequences)
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')

# Split training data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    X_train_padded, y_train, test_size=0.1, random_state=42
)

"""# DAN Model"""

# DAN Model
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 50

def create_dan_model():
    model = Sequential([
        Embedding(vocab_size, embedding_dim, input_length=max_length),
        GlobalAveragePooling1D(),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy']
    )
    return model

model = create_dan_model()

"""training"""

# Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=32
)

# Save the model
model.save('dan_model.h5')

"""testing the model"""

import seaborn as sns
import matplotlib.pyplot as plt

# Save the model
model.save('dan_model.h5')

# Evaluate the model on the test dataset
model = tf.keras.models.load_model('dan_model.h5')
y_pred_prob = model.predict(X_test_padded)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Extract confusion matrix values
tn, fp, fn, tp = conf_matrix.ravel()

# Print metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Annotate confusion matrix with TP, TN, FP, FN
conf_matrix_annot = [
    [f"TN: {tn}", f"FP: {fp}"],
    [f"FN: {fn}", f"TP: {tp}"]
]
# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=conf_matrix_annot, fmt='', cmap='Blues', xticklabels=['Control', 'Dementia'], yticklabels=['Control', 'Dementia'])
plt.title('Confusion Matrix__DANmodel')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()